{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yzLafeL1x-ar"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "en9RvaOqy4uG"
      },
      "outputs": [],
      "source": [
        "seed = 12\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PHxPMMGozOcc"
      },
      "outputs": [],
      "source": [
        "# load datasets\n",
        "domain1 = pd.read_json('data/domain1_train.json', lines=True)\n",
        "domain2 = pd.read_json('data/domain2_train.json', lines=True)\n",
        "test = pd.read_json('data/test_set.json', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lbpCZbrFz18h"
      },
      "outputs": [],
      "source": [
        "# random sample without replacement from label 0\n",
        "# label_0_rate: the rate of label 0 in the sampled dataframe, can be any number\n",
        "\n",
        "def under_sample(df, label_0_rate):\n",
        "\n",
        "    sampled_label_1 = df[df['label'] == 1]\n",
        "    label_1_count = len(df[df['label'] == 1])\n",
        "\n",
        "    #calculate the number of label 0 samples\n",
        "    sampled_label_0_count = int(label_0_rate * label_1_count)\n",
        "\n",
        "    total_label_0 = df[df['label'] == 0]\n",
        "\n",
        "    model_count = sampled_label_0_count // 7\n",
        "    sampled_label_0 = pd.DataFrame()\n",
        "\n",
        "    # use for loop to sample from each model\n",
        "    for i in range(7):\n",
        "        model = total_label_0[total_label_0['model'] == i]\n",
        "        sampled_model = model.sample(n=model_count, replace=False)\n",
        "        sampled_label_0 = pd.concat([sampled_label_0, sampled_model])\n",
        "    \n",
        "    # domain2_machine = train_data2[train_data2['label'] == 8].sample(4388)\n",
        "    # domain2_human = train_data2[train_data2['label'] == 1].sample(4300, replace = True)\n",
        "    # print(len(doamin2_machine), len(domain2_human))\n",
        "    # train_data = pd.concat(train_datal, domain2_machine[['text,'label']],domain2 human[['text','label']]]).sample(frac = 1)\n",
        "\n",
        "    # concatenate the sampled label 0 and sampled label 1\n",
        "    sampled_df = pd.concat([sampled_label_1, sampled_label_0])\n",
        "    \n",
        "    return sampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QF7bUOWO0hLD"
      },
      "outputs": [],
      "source": [
        "newdomain2 = under_sample(domain2,1)\n",
        "newdomain2 = newdomain2.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fAg2b5dVWatE"
      },
      "outputs": [],
      "source": [
        "domain2 = domain2[['text','label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RisleOg-0hHj"
      },
      "outputs": [],
      "source": [
        "# split data for validation\n",
        "train_domain1, valid_domain1 = train_test_split(domain1,test_size=0.2,random_state=12)\n",
        "train_domain2, valid_domain2 = train_test_split(domain2,test_size=0.2,random_state=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8uLxk_V-0g_w"
      },
      "outputs": [],
      "source": [
        "## input length should be the same while training neural network\n",
        "max_length = 100 \n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self,id):\n",
        "    txt = self.df.iloc[id,:]['text'][:max_length]\n",
        "    text_len = len(txt)\n",
        "    txt = torch.tensor(txt)\n",
        "    if text_len < max_length:\n",
        "      txt = F.pad(txt,(0,max_length - text_len),'constant', 5000)\n",
        "\n",
        "    label = torch.tensor(self.df.iloc[id,:]['label']).to(torch.int64)\n",
        "    return txt, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Bhx9RsxR2xeg"
      },
      "outputs": [],
      "source": [
        "batch_size = 256, # \n",
        "train_dm2 = MyDataset(train_domain1)\n",
        "train_loader = DataLoader(train_dm2, batch_size = 256, shuffle = True) \n",
        "valid_dm2 = MyDataset(valid_domain1)\n",
        "valid_loader = DataLoader(valid_dm2, batch_size = 256, shuffle = True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yqjtVfWZ2xIa"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, dict_size, emb_size, hidden_size):\n",
        "    super(MLP,self).__init__()\n",
        "    ## embedding layer =\n",
        "    self.embedding = nn.Embedding(dict_size, emb_size) # dict_size = 5000\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(emb_size, hidden_size),\n",
        "        nn.ReLU(), \n",
        "        nn.Linear(hidden_size, hidden_size), \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size), \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size,1) ## real value\n",
        "    )\n",
        "\n",
        "  def forward(self,text):\n",
        "    text_emb = self.embedding(text) # (batch_size, max_length, emb_size)\n",
        "    text_emb = text_emb.mean(dim = 1) # (batch_size, emb_size) mean\n",
        "    output = self.classifier(text_emb)\n",
        "    return output.squeeze() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Jd3XRyM42xA6"
      },
      "outputs": [],
      "source": [
        "GPU = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "model = MLP(dict_size = 5001, emb_size = 256, hidden_size = 256).to(GPU)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iRlPts0e2w5y"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader):\n",
        "  model.train()\n",
        "\n",
        "  batch_loss = 0.0\n",
        "  train_pred = []\n",
        "  train_targets = []\n",
        "\n",
        "  for batch in train_loader:\n",
        "\n",
        "    text, label = batch[0].to(GPU), batch[1].to(device=GPU, dtype=torch.long)\n",
        "\n",
        "    x_output = model.forward(text) \n",
        "\n",
        "    loss = criterion(x_output, label.float())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    batch_loss += loss.item()\n",
        "    train_pred += torch.round(torch.sigmoid(x_output)).tolist()\n",
        "    train_targets += label.tolist()\n",
        "\n",
        "    batch_loss /= len(train_loader)\n",
        "\n",
        "    train_fi = f1_score(train_targets, train_pred, average = 'macro')\n",
        "    return train_fi, batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TTx5T7bD_6Kb"
      },
      "outputs": [],
      "source": [
        "def valid_one_epoch(model, valid_loader):\n",
        "  model.eval() # not update gradients\n",
        "  batch_loss = 0.0\n",
        "  valid_pred = []\n",
        "  valid_target = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "      txt, label = batch[0].to(device=GPU, dtype=torch.long), batch[1].to(device=GPU, dtype=torch.long)\n",
        "\n",
        "      x_valid = model.forward(txt)\n",
        "      loss = criterion(x_valid, label.float())\n",
        "\n",
        "      batch_loss += loss.item()\n",
        "      valid_pred += torch.round(torch.sigmoid(x_valid)).tolist()\n",
        "      valid_target += label.tolist()\n",
        "\n",
        "      batch_loss /= len(valid_loader)\n",
        "      return batch_loss, valid_pred, valid_target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ibyvye2K_6BN",
        "outputId": "f359c687-dd80-4627-ee2e-e02db31b83a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, train loss: 0.015151219289810931.4f, train f1: 0.3385012919896641.4f, valid_loss: 0.04376106336712837.4f, valid_f1: 0.33506493506493507.4f\n",
            "epoch 2, train loss: 0.011586904525756836.4f, train f1: 0.33159268929503916.4f, valid_loss: 0.04274410009384155.4f, valid_f1: 0.3402061855670103.4f\n",
            "epoch 3, train loss: 0.011197035429907626.4f, train f1: 0.3385012919896641.4f, valid_loss: 0.043246038258075714.4f, valid_f1: 0.32275132275132273.4f\n",
            "epoch 4, train loss: 0.011151848269290611.4f, train f1: 0.3469387755102041.4f, valid_loss: 0.042859211564064026.4f, valid_f1: 0.3402061855670103.4f\n",
            "epoch 5, train loss: 0.01122124077843838.4f, train f1: 0.3435897435897436.4f, valid_loss: 0.0437910333275795.4f, valid_f1: 0.29863013698630136.4f\n",
            "epoch 6, train loss: 0.011261973224702428.4f, train f1: 0.34190231362467866.4f, valid_loss: 0.0429520383477211.4f, valid_f1: 0.34190231362467866.4f\n",
            "epoch 7, train loss: 0.011298893905076825.4f, train f1: 0.3402061855670103.4f, valid_loss: 0.04320754483342171.4f, valid_f1: 0.31733333333333336.4f\n",
            "epoch 8, train loss: 0.011348155678295698.4f, train f1: 0.3155080213903743.4f, valid_loss: 0.042885858565568924.4f, valid_f1: 0.33506493506493507.4f\n",
            "epoch 9, train loss: 0.011324452572181577.4f, train f1: 0.3118279569892473.4f, valid_loss: 0.04292040318250656.4f, valid_f1: 0.32275132275132273.4f\n",
            "epoch 10, train loss: 0.01120516511260486.4f, train f1: 0.32808398950131235.4f, valid_loss: 0.04252875968813896.4f, valid_f1: 0.3263157894736842.4f\n",
            "epoch 11, train loss: 0.011153203542115258.4f, train f1: 0.350253807106599.4f, valid_loss: 0.04199247434735298.4f, valid_f1: 0.33506493506493507.4f\n",
            "epoch 12, train loss: 0.011175900209145467.4f, train f1: 0.3298429319371728.4f, valid_loss: 0.04176830127835274.4f, valid_f1: 0.409261576971214.4f\n",
            "epoch 13, train loss: 0.011063319737793969.4f, train f1: 0.36542726679712983.4f, valid_loss: 0.04090733081102371.4f, valid_f1: 0.7603445047743869.4f\n",
            "epoch 14, train loss: 0.010684466752849642.4f, train f1: 0.7502439024390244.4f, valid_loss: 0.03922128677368164.4f, valid_f1: 0.7918220176448024.4f\n",
            "epoch 15, train loss: 0.010279576309391709.4f, train f1: 0.7964276975776854.4f, valid_loss: 0.038996536284685135.4f, valid_f1: 0.7222546777720286.4f\n",
            "epoch 16, train loss: 0.009768440098058983.4f, train f1: 0.7211200613732259.4f, valid_loss: 0.03643448278307915.4f, valid_f1: 0.6832824367708088.4f\n",
            "epoch 17, train loss: 0.0092612446331587.4f, train f1: 0.675.4f, valid_loss: 0.032612498849630356.4f, valid_f1: 0.7710761640456367.4f\n",
            "epoch 18, train loss: 0.009001041044954394.4f, train f1: 0.7375582479030754.4f, valid_loss: 0.04179669916629791.4f, valid_f1: 0.37922403003754696.4f\n",
            "epoch 19, train loss: 0.01090204520303695.4f, train f1: 0.40579681393457256.4f, valid_loss: 0.033843401819467545.4f, valid_f1: 0.6675324675324675.4f\n",
            "epoch 20, train loss: 0.009150053634018194.4f, train f1: 0.6623818225152935.4f, valid_loss: 0.036780912429094315.4f, valid_f1: 0.6190476190476191.4f\n",
            "epoch 21, train loss: 0.009587437403006632.4f, train f1: 0.6345149383661814.4f, valid_loss: 0.038305845111608505.4f, valid_f1: 0.5912083968780838.4f\n",
            "epoch 22, train loss: 0.009606541180219806.4f, train f1: 0.6570750317931326.4f, valid_loss: 0.033900946378707886.4f, valid_f1: 0.6744383213225943.4f\n",
            "epoch 23, train loss: 0.009857166985996434.4f, train f1: 0.5814861040307979.4f, valid_loss: 0.0332733690738678.4f, valid_f1: 0.7138294474608752.4f\n",
            "epoch 24, train loss: 0.008944124471945841.4f, train f1: 0.7080684214637194.4f, valid_loss: 0.10777077078819275.4f, valid_f1: 0.3155080213903743.4f\n",
            "epoch 25, train loss: 0.026458947384943726.4f, train f1: 0.33506493506493507.4f, valid_loss: 0.03270664066076279.4f, valid_f1: 0.7635727950392506.4f\n",
            "epoch 26, train loss: 0.0087349962015621.4f, train f1: 0.7327935222672064.4f, valid_loss: 0.03711200878024101.4f, valid_f1: 0.6149422236378759.4f\n",
            "epoch 27, train loss: 0.01019230631531262.4f, train f1: 0.5898473733029767.4f, valid_loss: 0.03917946666479111.4f, valid_f1: 0.5851339671564391.4f\n",
            "epoch 28, train loss: 0.010543936588725105.4f, train f1: 0.5444589165489534.4f, valid_loss: 0.041817378252744675.4f, valid_f1: 0.5269040997577823.4f\n",
            "epoch 29, train loss: 0.010523069100301773.4f, train f1: 0.601328903654485.4f, valid_loss: 0.04123523831367493.4f, valid_f1: 0.5124891378704679.4f\n",
            "epoch 30, train loss: 0.010884983617751325.4f, train f1: 0.5272727272727273.4f, valid_loss: 0.03926520049571991.4f, valid_f1: 0.5662615101289135.4f\n",
            "epoch 31, train loss: 0.01011544075168547.4f, train f1: 0.6090909090909091.4f, valid_loss: 0.03891075775027275.4f, valid_f1: 0.5598202895335292.4f\n",
            "epoch 32, train loss: 0.010131322946704801.4f, train f1: 0.5818181818181818.4f, valid_loss: 0.037677790969610214.4f, valid_f1: 0.5914219148428758.4f\n",
            "epoch 33, train loss: 0.009956504477829229.4f, train f1: 0.5976852108639784.4f, valid_loss: 0.03689226880669594.4f, valid_f1: 0.6136668079694786.4f\n",
            "epoch 34, train loss: 0.009815341136494621.4f, train f1: 0.5995375093518329.4f, valid_loss: 0.036894120275974274.4f, valid_f1: 0.6166863987742879.4f\n",
            "epoch 35, train loss: 0.009429614074894638.4f, train f1: 0.6430660409440251.4f, valid_loss: 0.03429808467626572.4f, valid_f1: 0.696022165050465.4f\n",
            "epoch 36, train loss: 0.009822997890534948.4f, train f1: 0.5940457312394496.4f, valid_loss: 0.03433578461408615.4f, valid_f1: 0.736638839699806.4f\n",
            "epoch 37, train loss: 0.009539273918652143.4f, train f1: 0.6744383213225944.4f, valid_loss: 0.03358498588204384.4f, valid_f1: 0.6397117422636711.4f\n",
            "epoch 38, train loss: 0.008767187595367432.4f, train f1: 0.7333333333333334.4f, valid_loss: 0.03362457826733589.4f, valid_f1: 0.7416666666666667.4f\n",
            "epoch 39, train loss: 0.008141251372509315.4f, train f1: 0.7627872498146775.4f, valid_loss: 0.030424589291214943.4f, valid_f1: 0.7730496453900709.4f\n",
            "epoch 40, train loss: 0.008229506797477847.4f, train f1: 0.7410695650777851.4f, valid_loss: 0.02904662862420082.4f, valid_f1: 0.7673121658886783.4f\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 40\n",
        "valid_f1s = []\n",
        "for epoch in range(num_epochs):\n",
        "  train_f1, train_loss = train_one_epoch(model, train_loader)\n",
        "  valid_loss, valid_targets, valid_pred = valid_one_epoch(model, valid_loader)\n",
        "  valid_f1 = f1_score(valid_targets, valid_pred, average = 'macro')\n",
        "  valid_f1s.append(valid_f1)\n",
        "  print(f\"epoch {epoch + 1}, train loss: {train_loss}.4f, train f1: {train_f1}.4f, valid_loss: {valid_loss}.4f, valid_f1: {valid_f1}.4f\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAUBVFU5YRt6"
      },
      "outputs": [],
      "source": [
        "mean(valid_f1s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNYvfAlU_5gy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
