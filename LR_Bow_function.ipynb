{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "domain1 = pd.read_json('data/domain1_train.json', lines=True)\n",
    "domain2 = pd.read_json('data/domain2_train.json', lines=True)\n",
    "test = pd.read_json('data/test_set.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample without replacement from label 0\n",
    "# label_0_rate: the rate of label 0 in the sampled dataframe, can be any number\n",
    "\n",
    "def under_sample(df, label_0_rate):\n",
    "\n",
    "    sampled_label_1 = df[df['label'] == 1]\n",
    "    label_1_count = len(df[df['label'] == 1])\n",
    "\n",
    "    #calculate the number of label 0 samples\n",
    "    sampled_label_0_count = int(label_0_rate * label_1_count)\n",
    "\n",
    "    total_label_0 = df[df['label'] == 0]\n",
    "\n",
    "    # calculate the number needed for each model\n",
    "    model_count = sampled_label_0_count // 7\n",
    "    sampled_label_0 = pd.DataFrame()\n",
    "\n",
    "    # use for loop to sample from each model\n",
    "    for i in range(7):\n",
    "        model = total_label_0[total_label_0['model'] == i]\n",
    "        sampled_model = model.sample(n=model_count, replace=False)\n",
    "        sampled_label_0 = pd.concat([sampled_label_0, sampled_model])\n",
    "    \n",
    "    # domain2_machine = train_data2[train_data2['label'] == 8].sample(4388)\n",
    "    # domain2_human = train_data2[train_data2['label'] == 1].sample(4300, replace = True)\n",
    "    # print(len(doamin2_machine), len(domain2_human))\n",
    "    # train_data = pd.concat(train_datal, domain2_machine[['text,'label']],domain2 human[['text','label']]]).sample(frac = 1)\n",
    "\n",
    "    # concatenate the sampled label 0 and sampled label 1\n",
    "    sampled_df = pd.concat([sampled_label_1, sampled_label_0])\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstrap human instances in domain2\n",
    "# %run -i explore.ipynb\n",
    "newdomain2 = under_sample(domain2,1)\n",
    "newdomain2 = newdomain2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for validation\n",
    "train_domain1, valid_domain1 = train_test_split(domain1,test_size=0.2,random_state=12)\n",
    "train_domain2, valid_domain2 = train_test_split(newdomain2,test_size=0.2,random_state=12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(method, train_df,valid_df, ngram, max_fea):\n",
    "    if method == 'count':\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram, max_features=max_fea, min_df=2)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram, max_features=max_fea)\n",
    "\n",
    "    ## transform train domain1's text to string \n",
    "    corpus = [' '.join(str(word_id) for word_id in document) for document in train_df['text']]\n",
    "    # learn the token dictionary and return document-term matrix\n",
    "    df_vec = vectorizer.fit_transform(corpus)\n",
    "    # get vectorized data\n",
    "    train_X = df_vec.toarray()\n",
    "\n",
    "    ## transform test domain1's text to string \n",
    "    corpus_test = [' '.join(str(word_id) for word_id in document) for document in valid_df['text']]\n",
    "    valid_X = vectorizer.transform(corpus_test)\n",
    "    valid_X.shape\n",
    "\n",
    "    return train_X, valid_X, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR(train_df, train_y, max_iter, pen, c, sam_solver):\n",
    "    lr = LogisticRegression(max_iter=max_iter, penalty=pen, C=c, solver=sam_solver)\n",
    "    ## Logistic regression in domain 1\n",
    "    lr.fit(train_df,train_y)\n",
    "    return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(lr,train_df,train_y,valid_df,valid_y):\n",
    "    ## Accuracy\n",
    "    print(\"training Accuracy: {:.4f}\".format(lr.score(train_df,train_y)))\n",
    "    print(\"valid Accuracy: {:.4f}\".format(lr.score(valid_df,valid_y)))\n",
    "    ## F1 score \n",
    "    print(\"Train F1 score: {:.4f}\".format(metrics.f1_score(train_y,lr.predict(train_df),average='macro')))\n",
    "    print(\"Test F1 score: {:.4f}\".format(metrics.f1_score(valid_y,lr.predict(valid_df),average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_lrs(train_domain1,train_domain2,valid_domain1,valid_domain2,\n",
    "             dm1_ngram,dm1_max_fea,dm1_max_iter,dm1_pen, dm1_c, dm1_sol,\n",
    "             dm2_ngram,dm2_max_fea,dm2_max_iter,dm2_pen, dm2_c, dm2_sol,\n",
    "             tt_ngram,tt_max_fea,tt_max_iter,tt_pen, tt_c, tt_sol\n",
    "             ):\n",
    "    ## domain 1 do LR\n",
    "    train_domain1_X, valid_domain1_X, v1 = vectorizer('count',train_domain1,valid_domain1,ngram=dm1_ngram,max_fea=dm1_max_fea)\n",
    "    lr1 = LR(train_domain1_X,train_domain1['label'],dm1_max_iter,dm1_pen,dm1_c,dm1_sol)\n",
    "    print(\"Domain1 LR\")\n",
    "    print_score(lr1,train_domain1_X,train_domain1['label'],valid_domain1_X,valid_domain1['label'])\n",
    "    \n",
    "    ## domain 2 do LR\n",
    "    train_domain2_X, valid_domain2_X, v2 = vectorizer('count',train_domain2,valid_domain2,ngram=dm2_ngram,max_fea=dm2_max_fea)\n",
    "    lr2 = LR(train_domain2_X,train_domain2['label'], dm2_max_iter ,dm2_pen, dm2_c,dm2_sol)\n",
    "    print(\"Domain2 LR\")\n",
    "    print_score(lr2,train_domain2_X,train_domain2['label'],valid_domain2_X,valid_domain2['label'])\n",
    "\n",
    "    ## combine dm1 dm2 datasets\n",
    "    total_train = pd.concat([train_domain1,train_domain2],axis=0,ignore_index=True)\n",
    "    total_valid = pd.concat([valid_domain1,valid_domain2],axis=0,ignore_index=True)\n",
    "    train_tt_X, valid_tt_X, v_tt = vectorizer('count',total_train,total_valid,ngram=tt_ngram,max_fea=tt_max_fea)\n",
    "\n",
    "    ## total dataset do LR\n",
    "    lr_tt = LogisticRegression(max_iter=tt_max_iter, penalty=tt_pen, C=tt_c, solver=tt_sol,class_weight='balanced')\n",
    "    lr_tt.fit(train_tt_X,total_train['label'])\n",
    "\n",
    "    print(\"Total LR\")\n",
    "    print_score(lr_tt,train_tt_X,total_train['label'],valid_tt_X,total_valid['label'])\n",
    "    \n",
    "    return v1, v2, v_tt, lr1, lr2, lr_tt, total_train,total_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(v1,v2,v3,lr_1,lr_2,lr_total,tt_valid):\n",
    "    corpus_tt_valid = [' '.join(str(word_id) for word_id in document) for document in tt_valid['text']]\n",
    "    \n",
    "    valid_md1 = v1.transform(corpus_tt_valid)\n",
    "    valid_md2 = v2.transform(corpus_tt_valid)\n",
    "    valid_md3 = v3.transform(corpus_tt_valid)\n",
    "    pred_md1 = lr_1.predict(valid_md1)\n",
    "    pred_md2 =  lr_2.predict(valid_md2)\n",
    "    pred_md3 =  lr_total.predict(valid_md3)\n",
    "\n",
    "    pred_voting = pd.DataFrame({'model1' : np.array(pred_md1),\n",
    "                            'model2' : np.array(pred_md2),\n",
    "                           'model3' : np.array(pred_md3)\n",
    "                            })\n",
    "    pred_voting['voting'] = pred_voting.mode(axis=1)\n",
    "    pred_voting = pd.concat([pred_voting,tt_valid['label']],axis=1)\n",
    "    print(\"Voting Valid Accuracy {:.4f}\".format(\n",
    "    (pred_voting['voting'] == pred_voting['label']).sum()/len(pred_voting)))\n",
    "    print(\"Voting Valid F1 score {:.4f}\".format(\n",
    "    format(metrics.f1_score(pred_voting['label'],pred_voting['voting'],average='macro'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(total_valid,v1,v2,v_tt,lr1,lr2,lr_total):\n",
    "    corpus_total_valid = [' '.join(str(word_id) for word_id in document) for document in total_valid['text']]\n",
    "\n",
    "    total_valid_length = pd.DataFrame(total_valid['text'].apply(lambda x: len(x)))\n",
    "    total_valid_length.rename(columns={'text':'length'},inplace=True)\n",
    "    total_valid_length = scaler.transform(total_valid_length)\n",
    "\n",
    "    total_valid_length = total_valid_length.values\n",
    "\n",
    "    ## transfer data \n",
    "    valid_md1 = v1.transform(corpus_total_valid)\n",
    "    valid_md2 = v2.transform(corpus_total_valid)\n",
    "    valid_md3 = v_tt.transform(corpus_total_valid)\n",
    "\n",
    "    ## get prediction in each lr\n",
    "    pred_md1 = lr1.predict(valid_md1)\n",
    "    pred_md2 = (lr2.predict_proba(valid_md2)[:,1] >= 0.95).astype(int)    \n",
    "    pred_md3 =  lr_total.predict(valid_md3)\n",
    "\n",
    "    ## using the stacking model to test validation set\n",
    "    X_val_meta = np.column_stack((pred_md1,pred_md2, pred_md3))\n",
    "\n",
    "    # Train the meta-model on the combined feature matrix and the target values\n",
    "    meta_model = LogisticRegressionCV(cv = 5,random_state=12)\n",
    "    meta_model.fit(X_val_meta, total_valid['label'])\n",
    "\n",
    "    y_val_meta = meta_model.predict(X_val_meta)\n",
    "    y_val_meta = pd.DataFrame(y_val_meta, columns=['meta'])\n",
    "\n",
    "    pred_stacking = pd.DataFrame()\n",
    "    pred_stacking['stacking'] = y_val_meta['meta'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "    pred_stacking = pd.concat([pred_stacking,total_valid['label']],axis=1)\n",
    "\n",
    "    print(\"Stacking valid accuracy {}\".format((pred_stacking['stacking'] == pred_stacking['label']).sum()/len(pred_stacking)))\n",
    "    print(\"Stacking valid F1 score {}\".format(metrics.f1_score(pred_stacking['label'],pred_stacking['stacking'],average='macro')))\n",
    "    return meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ensemble(v1,v2,v_tt,lr1,lr2,lr_total,meta_model):\n",
    "\n",
    "    corpus_test = [' '.join(str(word_id) for word_id in document) for document in test['text']]\n",
    "    \n",
    "    test_length = pd.DataFrame(test['text'].apply(lambda x: len(x)))\n",
    "    test_length.rename(columns={'text':'length'},inplace=True)\n",
    "    test_length = scaler.transform(test_length)\n",
    "\n",
    "    test_length = test_length.values\n",
    "\n",
    "\n",
    "    test_md1 = v1.transform(corpus_test)\n",
    "    test_md2 = v2.transform(corpus_test)\n",
    "    test_md3 = v_tt.transform(corpus_test)\n",
    "\n",
    "    ptest_md1 = lr1.predict(test_md1)\n",
    "    ptest_md2 = lr2.predict(test_md2)\n",
    "    ptest_md3 = lr_total.predict(test_md3)\n",
    "\n",
    "    X_test_meta = np.column_stack((ptest_md1,ptest_md2, ptest_md3))\n",
    "\n",
    "    y_test_meta = meta_model.predict(X_test_meta)\n",
    "\n",
    "    # majority voting\n",
    "    pre = pd.DataFrame({'model1' : np.array(ptest_md1),\n",
    "                            'model2' : np.array(ptest_md2),\n",
    "                           'model3' : np.array(ptest_md3)\n",
    "                            })\n",
    "   # pre['voting'] = pre.mode(axis=1)\n",
    "    pre['stacking'] = y_test_meta\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useless\n",
    "def process_LR(train_df,valid_df,ngram,max_fea,max_iter,pen,C,solver):\n",
    "    train_X, valid_X, vec = vectorizer('count',train_df,valid_df,ngram=ngram,max_fea=max_fea)\n",
    "    lr = LR(train_X,train_df['label'],max_iter,pen,C,solver)\n",
    "    print_score(lr,train_X,train_df['label'],valid_X,valid_df['label'])\n",
    "    return lr, vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_pen = 'l2'\n",
    "default_C = 1.0\n",
    "default_solver = 'lbfgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_solver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_4750/2143255812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m v1, v2, v_tt, lr1, lr2, lr_tt,total_train,total_valid = comb_lrs(train_domain1, train_domain2, valid_domain1, valid_domain2,\n\u001b[0;32m----> 2\u001b[0;31m          \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefault_solver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefault_solver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m            (1,2),20000,300,'l2',1.0,default_solver)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'default_solver' is not defined"
     ]
    }
   ],
   "source": [
    "v1, v2, v_tt, lr1, lr2, lr_tt,total_train,total_valid = comb_lrs(train_domain1, train_domain2, valid_domain1, valid_domain2,\n",
    "         (1,2),20000,150,'l2',1.0,default_solver,\n",
    "          (1,2),20000,400,'l2',1.0,default_solver,\n",
    "           (1,2),20000,300,'l2',1.0,default_solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    246\n",
       "4.0    245\n",
       "3.0    242\n",
       "6.0    242\n",
       "1.0    242\n",
       "0.0    237\n",
       "5.0    230\n",
       "Name: model, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_domain2.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Accuracy: 0.3934\n",
      "Test F1 score: 0.2824\n"
     ]
    }
   ],
   "source": [
    "valid_domain2_2 = valid_domain2[valid_domain2['model']==2.0]\n",
    "train_domain2_X, valid_domain2_X, v1 = vectorizer('count',train_domain2,valid_domain2_2,ngram=(1,2),max_fea=20000)\n",
    "print(\"valid Accuracy: {:.4f}\".format(lr2.score(valid_domain2_X,valid_domain2_2['label'])))\n",
    "print(\"Test F1 score: {:.4f}\".format(metrics.f1_score(valid_domain2_2['label'],lr2.predict(valid_domain2_X),average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    77\n",
       "0.0    70\n",
       "3.0    65\n",
       "6.0    65\n",
       "1.0    65\n",
       "4.0    62\n",
       "2.0    61\n",
       "Name: model, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_domain2.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Accuracy: 0.6615\n",
      "Test F1 score: 0.3981\n"
     ]
    }
   ],
   "source": [
    "valid_domain2_3 = valid_domain2[valid_domain2['model']==1.0]\n",
    "train_domain2_X, valid_domain2_X, v1 = vectorizer('count',train_domain2,valid_domain2_3,ngram=(1,2),max_fea=20000)\n",
    "print(\"valid Accuracy: {:.4f}\".format(lr2.score(valid_domain2_X,valid_domain2_3['label'])))\n",
    "print(\"Test F1 score: {:.4f}\".format(metrics.f1_score(valid_domain2_3['label'],lr2.predict(valid_domain2_X),average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine dm1 dm2 datasets\n",
    "total_train = pd.concat([train_domain1,train_domain2],axis=0,ignore_index=True)\n",
    "total_valid = pd.concat([valid_domain1,valid_domain2],axis=0,ignore_index=True)\n",
    "train_tt_X, valid_tt_X, v_tt = vectorizer('count',total_train,total_valid,ngram=(1,2),max_fea=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain1 Model Valid_domain2 LR\n",
      "training Accuracy: 0.9892\n",
      "valid Accuracy: 0.8441\n",
      "Train F1 score: 0.9892\n",
      "Test F1 score: 0.8431\n"
     ]
    }
   ],
   "source": [
    "train_domain1_X, valid_domain2_X, v1 = vectorizer('count',train_domain1,total_valid,ngram=(1,2),max_fea=20000)\n",
    "lr1 = LR(train_domain1_X,train_domain1['label'],150,'l2',1.0,default_solver)\n",
    "print(\"Domain1 Model Valid_domain2 LR\")\n",
    "print_score(lr1,train_domain1_X,train_domain1['label'],valid_domain2_X,total_valid['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_stacking():\n",
    "    stacking_model = stacking(total_valid,v1,v2,v_tt,lr1,lr2,lr_tt)\n",
    "    test_pred = test_ensemble(v1,v2,v_tt,lr1,lr2,lr_tt,stacking_model)\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking valid accuracy 0.8586134453781512\n",
      "Stacking valid F1 score 0.8583901894267935\n"
     ]
    }
   ],
   "source": [
    "stacking_model = stacking(total_valid,v1,v2,v_tt,lr1,lr2,lr_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07769313, -0.00181605,  0.08108293])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking_model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_58414/1551457512.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_stacking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_58414/2729640950.py\u001b[0m in \u001b[0;36mcall_stacking\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_stacking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstacking_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_tt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_tt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_tt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_tt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstacking_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_58414/2910211968.py\u001b[0m in \u001b[0;36mstacking\u001b[0;34m(total_valid, v1, v2, v_tt, lr1, lr2, lr_total)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_valid_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_valid_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtotal_valid_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_valid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_valid_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_valid_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \"\"\"\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         X = self._validate_data(X, copy=self.copy, dtype=FLOAT_DTYPES,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "test_pred = call_stacking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Valid Accuracy 0.8553\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown format code 'f' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_58414/195813710.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvoting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_tt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_tt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gk/b497571d49ndglvgyk5sb53h0000gn/T/ipykernel_58414/1485457552.py\u001b[0m in \u001b[0;36mvoting\u001b[0;34m(v1, v2, v3, lr_1, lr_2, lr_total, tt_valid)\u001b[0m\n\u001b[1;32m     17\u001b[0m     print(\"Voting Valid Accuracy {:.4f}\".format(\n\u001b[1;32m     18\u001b[0m     (pred_voting['voting'] == pred_voting['label']).sum()/len(pred_voting)))\n\u001b[0;32m---> 19\u001b[0;31m     print(\"Voting Valid F1 score {:.4f}\".format(\n\u001b[0m\u001b[1;32m     20\u001b[0m     format(metrics.f1_score(pred_voting['label'],pred_voting['voting'],average='macro'))))\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown format code 'f' for object of type 'str'"
     ]
    }
   ],
   "source": [
    "voting(v1,v2,v_tt,lr1,lr2,lr_tt,total_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>voting</th>\n",
       "      <th>stacking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model1  model2  model3  voting  stacking\n",
       "0       1       1       1       1         1\n",
       "1       1       0       0       0         0\n",
       "2       1       1       1       1         1\n",
       "3       0       0       0       0         0\n",
       "4       0       1       0       0         0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': range(len(test)), 'class': test_pred['voting']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/max_iter_15050100_Undsamp_(1,2)_voting_20000_LR_.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'penalty': ['l1', 'l2'],  # or 'elasticnet' if using both L1 and L2 penalties\n",
    "    'C'       : np.logspace(-3,3,7),\n",
    "    'solver': [ 'saga']  # 'liblinear' for small datasets, 'saga' for large datasets\n",
    "}\n",
    "\n",
    "def LR(train_df, train_y,iter):\n",
    "    lr = LogisticRegression(max_iter=iter)\n",
    "    clf = GridSearchCV(lr, param_grid=parameters, scoring='accuracy',cv=5)\n",
    "    clf.fit(train_df,train_y)\n",
    "    return lr \n",
    "\n",
    "def comb_lrs(train_domain1,train_domain2,valid_domain1,valid_domain2,\n",
    "             dm1_ngram,dm1_max_fea,dm1_iter,\n",
    "             dm2_ngram,dm2_max_fea,dm2_iter,\n",
    "             tt_ngram,tt_max_fea,tt_iter\n",
    "             ):\n",
    "    ## domain 1 do LR\n",
    "    train_domain1_X, valid_domain1_X, v1 = vectorizer('count',train_domain1,valid_domain1,ngram=dm1_ngram,max_fea=dm1_max_fea)\n",
    "    lr1 = LR(train_domain1_X,train_domain1['label'],dm1_iter)\n",
    "    print(\"Domain1 LR\")\n",
    "    print_score(lr1,train_domain1_X,train_domain1['label'],valid_domain1_X,valid_domain1['label'])\n",
    "    \n",
    "    ## domain 2 do LR\n",
    "    train_domain2_X, valid_domain2_X, v2 = vectorizer('count',train_domain2,valid_domain2,ngram=dm2_ngram,max_fea=dm2_max_fea)\n",
    "    lr2 = LR(train_domain2_X,train_domain2['label'],dm2_iter)\n",
    "    print(\"Domain2 LR\")\n",
    "    print_score(lr2,train_domain2_X,train_domain2['label'],valid_domain2_X,valid_domain2['label'])\n",
    "\n",
    "    ## combine dm1 dm2 datasets\n",
    "    total_train = pd.concat([train_domain1,train_domain2],axis=0,ignore_index=True)\n",
    "    total_valid = pd.concat([valid_domain1,valid_domain2],axis=0,ignore_index=True)\n",
    "    train_tt_X, valid_tt_X, v_tt = vectorizer('count',total_train,total_valid,ngram=tt_ngram,max_fea=tt_max_fea)\n",
    "\n",
    "    ## total dataset do LR\n",
    "    lr_tt = LR(train_tt_X,total_train['label'])\n",
    "    print(\"Total LR\")\n",
    "    print_score(lr_tt,train_tt_X,total_train['label'],valid_tt_X,total_valid['label'])\n",
    "    \n",
    "    return v1, v2, v_tt, lr1, lr2, lr_tt, total_train,total_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample with replacement from label 1\n",
    "# label_1_rate: the rate of label 1 in the sampled dataframe, should be between 0 and 1\n",
    "\n",
    "def rated_sample(df, label_1_rate):\n",
    "\n",
    "    sampled_label_0 = df[df['label'] == 0]\n",
    "    label_0_count = len(df[df['label'] == 0])\n",
    "\n",
    "    #calculate the number of label 1 samples\n",
    "    sampled_label_1_count = int(label_1_rate * label_0_count)\n",
    "    \n",
    "    # random sample with replacement from label 1\n",
    "    sampled_label_1 = df[df['label'] == 1].sample(n=sampled_label_1_count, replace=True)\n",
    "\n",
    "    # concatenate the sampled label 0 and sampled label 1\n",
    "    sampled_df = pd.concat([sampled_label_0, sampled_label_1])\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12750\n",
       "1    12750\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_dm2 = rated_sample(domain2,1)\n",
    "over_dm2.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
